import os
import re
import streamlit as st
from collections import Counter
from datetime import datetime
from nltk.tokenize.punkt import PunktSentenceTokenizer
from nltk.tokenize import wordpunct_tokenize
from nltk.corpus import stopwords
import textstat
from docx import Document
from striprtf.striprtf import rtf_to_text
import nltk
import requests

# â”€â”€ LICENSE / PRO CHECK SETUP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# These pull from Streamlit Community Cloud Secrets, or from .streamlit/secrets.toml locally
ACCESS_TOKEN = st.secrets["GUMROAD_ACCESS_TOKEN"]
PERMALINK    = st.secrets["GUMROAD_PRODUCT_PERMALINK"]

if "pro_unlocked" not in st.session_state:
    st.session_state.pro_unlocked = False

def verify_license(license_key: str) -> bool:
    """Verify a Gumroad license key via their API."""
    resp = requests.post(
        "https://api.gumroad.com/v2/licenses/verify",
        data={
            "product_permalink": PERMALINK,
            "license_key": license_key,
            "access_token": ACCESS_TOKEN
        },
        timeout=10
    ).json()
    return resp.get("success", False)

# â”€â”€ NLTK SETUP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
nltk.download('punkt')
nltk.download('stopwords')

# â”€â”€ CONSTANTS & PRESETS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
FILLER_WORDS = ["just", "really", "very", "that", "actually",
                "like", "maybe", "somewhat", "perhaps", "quite"]

CLICHES = [
    "needle in a haystack", "cold sweat", "chill ran down",
    "time stood still", "dead silence", "at the end of the day",
    "low-hanging fruit", "the calm before the storm", "head over heels",
    "in the nick of time", "plenty of fish in the sea", "easy as pie",
    "scared stiff", "raining cats and dogs", "think outside the box",
    "every cloud has a silver lining", "pushing up daisies",
    "barking up the wrong tree", "blood ran cold", "fit as a fiddle"
]

STYLE_PRESETS = {
    "Gritty": {
        "emphasis": "ClichÃ© detection, passive voice, long sentences",
        "note": "Highlights harshness and realism â€” cracks down on clichÃ©s and overwritten prose."
    },
    "Snappy": {
        "emphasis": "Filler words, punchy structure, dialogue ratio",
        "note": "Focuses on rhythm, minimalism, and active scenes."
    },
    "Poetic": {
        "emphasis": "Flow, sentence variety, rhythm",
        "note": "Tolerates longer prose, flags broken rhythm or repetition."
    },
    "Technical": {
        "emphasis": "Passive voice, clarity, redundancy",
        "note": "Suited for nonfiction or precise procedural tone."
    },
    "Sparse": {
        "emphasis": "Minimal filler, clarity, clean structure",
        "note": "Zero fluff. Cuts verbosity. Favors clean delivery."
    }
}

# â”€â”€ CORE FUNCTIONS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def clean_text(text: str) -> str:
    text = text.replace("â€œ", "\"").replace("â€", "\"")
    text = text.replace("â€˜", "'").replace("â€™", "'")
    text = text.replace("--", "â€”")
    return ' '.join(text.split())

def suggest_improvements(text: str) -> str:
    tokenizer = PunktSentenceTokenizer()
    sentences = tokenizer.tokenize(text)
    suggestions = []
    for i, sentence in enumerate(sentences):
        issues = []
        if len(wordpunct_tokenize(sentence)) > 30:
            issues.append("âš ï¸ Consider breaking this long sentence into two or more.")
        if re.search(r'\b(was|were|is being|are being|has been|have been|had been)\b\s+\w+ed\b', sentence):
            issues.append("ğŸ’¡ Try rephrasing in active voice.")
        filler_count = sum(sentence.lower().count(fw) for fw in FILLER_WORDS)
        if filler_count > 2:
            issues.append("âœ‚ï¸ This line may be padded with filler words.")
        if issues:
            suggestions.append(f"\nSentence {i+1}:\nâ€œ{sentence.strip()}â€\n" + "\n".join(issues))
    return "\n".join(suggestions) if suggestions else "âœ… No smart suggestions needed â€” looking solid!"

def detect_passive_voice(text: str):
    tokenizer = PunktSentenceTokenizer()
    sentences = tokenizer.tokenize(text)
    passive = []
    pattern = re.compile(r'\b(was|were|is being|are being|has been|have been|had been)\b\s+\w+ed\b', re.IGNORECASE)
    for i, sentence in enumerate(sentences):
        if pattern.search(sentence):
            passive.append((i+1, sentence.strip()))
    return passive

def analyze_text(text: str, style="None") -> str:
    report = []
    if style in STYLE_PRESETS:
        report += [
            f"ğŸ¨ Style Preset: {style}",
            f"ğŸ” Focus: {STYLE_PRESETS[style]['emphasis']}",
            f"_Note: {STYLE_PRESETS[style]['note']}_", ""
        ]
    report += [
        "ğŸ“Š Analysis Report:",
        f"â€¢ Total words: {len(text.split())}",
        f"â€¢ Total sentences: {textstat.sentence_count(text)}",
        f"â€¢ Avg sentence length: {textstat.words_per_sentence(text):.2f} words",
        f"â€¢ Reading grade level: {textstat.flesch_kincaid_grade(text):.2f}", ""
    ]
    report.append("ğŸ” Common Filler Words Found:")
    for w in FILLER_WORDS:
        c = text.lower().split().count(w)
        if c > 0:
            report.append(f"   - {w}: {c}")
    report.append("\nâš ï¸ Long Sentences (over 30 words):")
    sentences = PunktSentenceTokenizer().tokenize(text)
    for i, s in enumerate(sentences):
        wc = len(wordpunct_tokenize(s))
        if wc > 30:
            report.append(f"\nSentence {i+1} ({wc} words):\n{s}")
    report.append("\nğŸ“ˆ Top 5 Most Frequent Words (excl. stopwords):")
    words = [w for w in wordpunct_tokenize(text.lower())
             if w.isalpha() and w not in stopwords.words('english')]
    for w, c in Counter(words).most_common(5):
        report.append(f"   - {w}: {c}")
    report.append("\nğŸ•µï¸ Potential Passive Voice Sentences:")
    passive = detect_passive_voice(text)
    report += [f"\nSentence {n}:\n{s}" for n, s in passive] if passive else ["   None found âœ…"]
    report += ["\nğŸ¤– Smart Suggestions:", suggest_improvements(text)]
    return "\n".join(report)

def extract_dialogue(text: str) -> str:
    matches = re.findall(r'[â€œ"]([^â€œâ€"]+)[â€"]', text)
    return "\n".join(m.strip() for m in matches if m.strip())

def dialogue_by_character(text: str) -> str:
    matches = re.findall(
        r'"[^"]+?"\s+(?:said|asked|replied|whispered|shouted|cried|muttered|yelled|snapped|called)\s+([A-Z][a-zA-Z]*)',
        text)
    if not matches:
        return "No named characters found."
    counts = Counter(matches).most_common()
    return "ğŸ§ Dialogue by Character:\n" + "\n".join(f"   - {n}: {c} lines" for n, c in counts)

def find_cliches(text: str) -> str:
    found = [f"â€¢ {ph}" for ph in CLICHES if ph in text.lower()]
    return "\nğŸ’£ ClichÃ©s Found:\n" + "\n".join(found) if found else "âœ… No clichÃ©s found!"

def load_docx(file) -> str:
    return "\n".join(p.text for p in Document(file).paragraphs)

def export_full_report(text: str, style="None") -> str:
    now = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    parts = [
        f"ğŸ—‚ Plush: Full Report\nGenerated: {now}",
        analyze_text(text, style), "="*50,
        dialogue_by_character(text), "="*50,
        "ğŸ—£ Extracted Dialogue:\n" + extract_dialogue(text), "="*50,
        find_cliches(text), "="*50,
        "ğŸ–¤ Crafted by Bastian â€” Plush Edition for writtenbybc.com"
    ]
    return "\n\n".join(parts)

# â”€â”€ STREAMLIT APP LAYOUT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def main():
    st.set_page_config(
        page_title="Plush: The Writer Toolkit",
        page_icon="plush-favicon.png",  # your crocodile icon file in repo
        layout="centered"
    )
    st.title("ğŸ§  Plush: The Writer Toolkit")
    st.markdown("*Tribute to Bodhi Crocodile â€” soft on the surface, sharp on the scene.*")

    # â”€â”€ PRO UNLOCK SIDEBAR â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    with st.sidebar:
        st.header("ğŸ” Plush Pro Unlock")
        if not st.session_state.pro_unlocked:
            key = st.text_input("Enter your license key")
            if st.button("Verify Key"):
                if verify_license(key):
                    st.session_state.pro_unlocked = True
                    st.success("ğŸ‰ Plush Pro unlocked!")
                else:
                    st.error("âŒ Invalid keyâ€”try again or purchase above.")
            st.markdown("[Buy Plush Pro for $19](https://bradleyc51.gumroad.com/l/ccvuo)")
        else:
            st.info("âœ… Plush Pro features unlocked!")

    # â”€â”€ STYLE PRESET â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if st.session_state.pro_unlocked:
        style = st.selectbox(
            "ğŸ¨ Choose a Writing Style Preset",
            ["None"] + list(STYLE_PRESETS.keys())
        )
    else:
        st.markdown("### ğŸ¨ Writing Style Presets (Plush Pro only)")
        st.info("ğŸ”’ Unlock Plush Pro to access custom style presets.")
        style = "None"

    # â”€â”€ FILE UPLOADER â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if st.session_state.pro_unlocked:
        uploaded_files = st.file_uploader(
            "ğŸ“‚ Upload one or more `.txt`, `.docx`, or `.rtf` files",
            type=["txt", "docx", "rtf"],
            accept_multiple_files=True
        )
    else:
        uploaded_file = st.file_uploader(
            "ğŸ“‚ Upload a single `.txt`, `.docx`, or `.rtf` file",
            type=["txt", "docx", "rtf"]
        )
        st.info("ğŸ”’ Upgrade to Plush Pro to analyze entire manuscripts at once.")
        uploaded_files = [uploaded_file] if uploaded_file else []

    # â”€â”€ PROCESS UPLOADED CONTENT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    raw_text = ""
    for uf in uploaded_files:
        if uf is None:
            continue
        ext = os.path.splitext(uf.name)[1].lower()
        if ext == ".txt":
            raw_text += uf.read().decode("utf-8") + "\n"
        elif ext == ".docx":
            raw_text += load_docx(uf) + "\n"
        elif ext == ".rtf":
            raw_text += rtf_to_text(uf.read().decode("utf-8")) + "\n"

    if raw_text:
        cleaned = clean_text(raw_text)

        # â”€â”€ CLEAN ONLY â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if st.button("ğŸ§¼ Clean Only"):
            st.subheader("âœ… Cleaned Text")
            st.text_area("Cleaned Output", cleaned, height=400)
            st.download_button("â¬‡ï¸ Download Cleaned File",
                               cleaned, file_name="cleaned_output.txt")

        # â”€â”€ ANALYZE ONLY â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if st.button("ğŸ” Analyze Only"):
            st.subheader("ğŸ“Š Analysis Report + ğŸ¤– Smart Suggestions")
            report = analyze_text(raw_text, style)
            st.text_area("Report", report, height=800)
            st.download_button("â¬‡ï¸ Download Report",
                               report, file_name="analysis_report.txt")

        # â”€â”€ CLEAN & ANALYZE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if st.button("ğŸ§¼ + ğŸ” Clean & Analyze"):
            st.subheader("âœ… Cleaned Text")
            st.text_area("Cleaned Output", cleaned, height=400)
            st.download_button("â¬‡ï¸ Download Cleaned File",
                               cleaned, file_name="cleaned_output.txt")
            st.subheader("ğŸ“Š Analysis Report + ğŸ¤– Smart Suggestions")
            report = analyze_text(cleaned, style)
            st.text_area("Report", report, height=800)
            st.download_button("â¬‡ï¸ Download Report",
                               report, file_name="analysis_report.txt")

        # â”€â”€ DIALOGUE ONLY â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if st.button("ğŸ—£ Extract Dialogue Only"):
            st.subheader("ğŸ—£ Dialogue Only")
            dialogue = extract_dialogue(raw_text)
            if dialogue:
                st.text_area("Extracted Dialogue", dialogue, height=600)
                st.download_button("â¬‡ï¸ Download Dialogue",
                                   dialogue, file_name="dialogue_only.txt")
            else:
                st.warning("No dialogue found in this manuscript.")

        # â”€â”€ DIALOGUE BY CHARACTER â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if st.button("ğŸ§ Dialogue by Character"):
            st.subheader("ğŸ§ Dialogue by Character")
            char_report = dialogue_by_character(raw_text)
            st.text_area("Speaker Breakdown", char_report, height=400)
            st.download_button("â¬‡ï¸ Download Character Dialogue Report",
                               char_report, file_name="character_dialogue.txt")

        # â”€â”€ CLICHÃ‰ BUSTER â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if st.button("ğŸ’£ ClichÃ© Buster"):
            st.subheader("ğŸ’£ ClichÃ© Buster")
            cpl = find_cliches(raw_text)
            st.text_area("ClichÃ© Report", cpl, height=400)
            st.download_button("â¬‡ï¸ Download ClichÃ© Report",
                               cpl, file_name="cliches_found.txt")

        # â”€â”€ EXPORT FULL REPORT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if st.session_state.pro_unlocked:
            if st.button("ğŸ“¦ Export Full Report"):
                full = export_full_report(raw_text, style)
                st.text_area("Full Report Preview", full, height=800)
                st.download_button("â¬‡ï¸ Download Full Report",
                                   full, file_name="plush_full_report.txt")
        else:
            st.info("ğŸ”’ Unlock Plush Pro to export polished full reports!")

    st.markdown("---")
    st.markdown("ğŸ–¤ *Crafted by Bastian â€” Plush Edition for* [**writtenbybc.com**](https://www.writtenbybc.com)")

if __name__ == "__main__":
    main()
